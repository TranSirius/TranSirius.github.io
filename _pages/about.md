---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span style="color: rgb(160,0,0)"><strong>Note:</strong> My main page has been moved to <a href="https://zijun-yao.github.io">zijun-yao.github.io</a>.</span>

I am a Ph.D. candidate in the Knowledge Engineering Group (KEG), Department of Computer Science and Technology, Tsinghua University (THU, Sep 2023 – Jun 2026, expected), advised by Prof. [Juanzi Li](https://keg.cs.tsinghua.edu.cn/persons/ljz/). 
Before that, I finished my education as a master student in THU-KEG, also advised by Prof. Juanzi Li from Sep 2020 to Jun 2023.
I received my bachelor's degree in Computer Science and Technology from Beijing University of Posts and Telecommunications (BUPT, Sep 2016 – Jun 2020).

My research sits at the intersection of **large language models (LLMs)**, **knowledge engineering**, and **reasoning**. 
I have worked as a research intern at Zhipu.AI on building and post-training LLMs, where I work closely with Dr. [Xin Lv](https://davidlvxin.github.io/), and I was a visiting scholar at the NExT++ Research Centre, National University of Singapore (Mar 2025 – Sep 2025) hosted by Prof. [Chua Tat-Seng](https://www.chuatatseng.com/). 
Earlier, I spent time in THU-KEG on graph learning, supervised by Prof. [Jie Tang](https://keg.cs.tsinghua.edu.cn/jietang/).
I also worked with Prof. [Bin Wu](http://web.evolbio.mpg.de/~bin.wu/welcome.html) on evolutionary game theory on graph.

You can find my publications on **[Google Scholar](https://scholar.google.com/citations?user=B4LmHSUAAAAJ)**.


## Detailed Research Interests

I am dedicated to exploring how to (1) establish the science of large language models (Science of LLMs); and (2) enable LLMs with knowledge and reasoning skills to solve scientific tasks (LLMs for Science).

**1) Science of LLMs**  
I aim to build a *systematic science* of LLMs that connects internal model mechanics to observable behaviors.

- **Microscopic perspective:** Probe how hidden states, neurons, and sparse features arise and correspond to specific model behaviors; develop tools (e.g., sparse autoencoders) to interpret and **steer** model internals.  
- **Macroscopic perspective:** Study how architectural and training choices (data, objectives, RL, distillation) shape emergent capabilities such as implicit reasoning and robustness.

**2) LLMs for Science**  
I explore how LLMs can *assist and automate* the scientific workflow.

- **Knowledge-intensive QA and retrieval:** Combine parametric and retrieved knowledge for fact-seeking, multi-hop reasoning, and conflict resolution.  
- **Agentic research assistants:** Build multi-agent systems and evaluation protocols for end-to-end scientific inquiry (problem formulation → evidence gathering → reasoning → reporting).  
- **Learning from real-world feedback:** Use verifiable signals and reward models to help LLMs improve their research skills and factual reliability over time.


**Selected publications**


1. **SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation**  
   <span style="color: rgb(100,0,0)">**Zijun Yao**</span>\*, Weijian Qi\*, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, Juanzi Li  
   *ACL, 2025.* <span style="color: rgb(160,0,0)">**Oral Presentation (2.9% in Submission)**</span>

2. **VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering**  
   <span style="color: rgb(100,0,0)">**Zijun Yao**</span>\*, Yuanyong Chen\*, Xin Lv, Shulin Cao, Amy Xin, Jifan Yu, Hailong Jin, Jianjun Xu, Peng Zhang, Lei Hou, Juanzi Li  
   *Demo of ACL, 2023.* <span style="color: rgb(160,0,0)">**Best Demo Award**</span>

3. **KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding**  
   <span style="color: rgb(100,0,0)">**Zijun Yao**</span>\*, Yantao Liu\*, Xin Lv, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li  
   *Findings of ACL, 2023.*

4. **Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models**  
   Yantao Liu\*, <span style="color: rgb(100,0,0)">**Zijun Yao**</span>\*, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li  
   *LREC-COLING, 2024.* \[<a href="https://arxiv.org/abs/2405.11876">arXiv</a>\]

5. **RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style**  
   Yantao Liu, <span style="color: rgb(100,0,0)">**Zijun Yao**</span>, Rui Min, Yixin Cao, Lei Hou, Juanzi Li  
   *ICLR, 2025.* <span style="color: rgb(160,0,0)">**Oral Presentation (1.2% in Submission)**</span>

6. **Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking**  
   Xiaokang Zhang\*, <span style="color: rgb(100,0,0)">**Zijun Yao**</span>\*, Jing Zhang, Kaifeng Yun, Jifan Yu, Juanzi Li, Jie Tang  
   *ACL, 2024.* \[<a href="https://arxiv.org/abs/2405.11874">arXiv</a>\]

7. **LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language Models via Sparse Auto-Encoder**  
   Yi Jing, <span style="color: rgb(100,0,0)">**Zijun Yao**</span>, Lingxu Ran, Hongzhu Guo, Xiaozhi Wang, Lei Hou, Juanzi Li  
   *EMNLP, 2025.*

8. **How does Transformer Learn Implicit Reasoning?**  
   Jiaran Ye\*, <span style="color: rgb(100,0,0)">**Zijun Yao**</span>\*, Zhidian Huang, Liangming Pan, Jinxin Liu, Yushi Bai, Amy Xin, Liu Weichuan, Xiaoyin Che, Lei Hou, Juanzi Li  
   *NeurIPS, 2025.* <span style="color: rgb(160,0,0)">**Spotlight (3.5% in Submission)**</span>

9. **Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making**  
   <span style="color: rgb(100,0,0)">**Zijun Yao**</span>, Chengjiang Li, Tiansi Dong, Xin Lv, Jifan Yu, Lei Hou, Juanzi Li, Yichi Zhang, Zelin Dai  
   *ACL-IJCNLP, 2021.* \[<a href="https://arxiv.org/abs/2105.14467">arXiv</a>\]

<sup>\* Equal contribution</sup>




<!-- 
I am a third year master student at the [Department of Computer Science and Technology](http://www.cs.tsinghua.edu.cn/) in Tsinghua University, supervised by [Prof. Juanzi Li](http://keg.cs.tsinghua.edu.cn/persons/ljz/).
I received my bachelor's degree in Computer Science and Technology from [Beijing University of Posts and Telecommunications](https://www.bupt.edu.cn/).
I am currently working on knowledge base question answering and question generation.
I previously worked on evoluationary game theory and graph neural networks.

What's New
=====


Academic Service
=====

- Program Committee Member/Reviewer:  WWW-DL4G 2020, EMNLP 2022, ACL 2023, TKDE, and SMC.
- Secondary Reviewer:  IJCAI/AAAI/EMNLP 2021, AAAI/COLING 2022, ACL Rolling Review.

Publications & Preprints
======

2021
-----

<style>
td, th {
   border: none!important;
}
</style>

<table style="border: none!important;">
	  <tbody><tr><td style="width:230px; height:110px" valign="middle" align="middle">
	    <img src="http://transirius.github.io/images/pub/kat.png" width="250">
	  </td>
	  <td style="width:10px">
	  </td>
	  <td valign="middle">
	    <div>
	    	<b>
        	Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making
        </b>
        <br>
		<i>
        	ACL 2021
        </i>
        <br>
	    	<b>Zijun Yao</b>, Chengjiang Li, Tiansi Dong, Xin Lv, Jifan Yu, Lei Hou, Juanzi Li, Yichi Zhang and Zelin Dai
        <br>
		[<a href="https://arxiv.org/abs/2106.04174">PDF</a>]
        [<a href="https://github.com/THU-KEG/HIF-KAT">Code</a>]
		[<a href="http://transirius.github.io/files/kat.pdf">Slide</a>]
        <br>
			We propose to decouple the representation learning stage and the decision making stage to fully utilize unlabeled data for entity matching task.
		</div>
	</td></tr></tbody>
</table>

<table style="border: none!important;">
	  <tbody><tr><td style="width:230px; height:110px" valign="middle" align="middle">
	    <img src="http://transirius.github.io/images/pub/symbiosis.png" width="250">
	  </td>
	  <td style="width:10px">
	  </td>
	  <td valign="middle">
	    <div>
		<b>
			Graph Symbiosis Learning
        </b>
        <br>
		<i>
        	Arxiv Preprint
        </i>
        <br>
	    	Liang Zeng, Jin Xu, <b>Zijun Yao</b>, Yanqiao Zhu, Jian Li
        <br>
		[<a href="https://arxiv.org/abs/2106.05455">PDF</a>]
        <br>
			We introduce a framework for learning from multiple generated graph views, named graph symbiosis learning (GraphSym). In GraphSym, graph neural networks (GNN) developed in multiple generated graph views can adaptively exchange parameters with each other and fuse information stored in linkage structures and node features.
		</div>
	</td></tr></tbody>
</table> -->
